{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b2f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994d58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa522e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples: 182552\n",
      "dev examples: 22840\n",
      "test examples: 22754\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "def build_training_data(words):\n",
    "    \"\"\"\n",
    "    Return X, Y tuple of training data and labels given words.\n",
    "\n",
    "    X will contain one row for each example. Each example will contain `context_size`\n",
    "    elements representing character indices.\n",
    "\n",
    "    Y will contain a character index label for each example.\n",
    "    \"\"\"\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for word in words:\n",
    "        context = [0] * context_size\n",
    "        for ch in word:\n",
    "            ich = stoi[ch]\n",
    "            xs.append(context)\n",
    "            ys.append(ich)\n",
    "            context = context[1:] + [ich]\n",
    "        xs.append(context)\n",
    "        ys.append(0)\n",
    "    assert len(xs) == len(ys)\n",
    "    X = torch.tensor(xs)\n",
    "    Y = torch.tensor(ys)\n",
    "    return X, Y\n",
    "\n",
    "train_cutoff = int(0.8 * len(words))\n",
    "dev_cutoff = int(0.9 * len(words))\n",
    "\n",
    "X_train, Y_train = build_training_data(words[:train_cutoff])\n",
    "X_dev, Y_dev = build_training_data(words[train_cutoff:dev_cutoff])\n",
    "X_test, Y_test = build_training_data(words[dev_cutoff:])\n",
    "\n",
    "print(\"training examples:\", len(Y_train))\n",
    "print(\"dev examples:\", len(Y_dev))\n",
    "print(\"test examples:\", len(Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a686617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8108\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of characters in our alphabet (the very first one is the terminator character).\n",
    "vocab_size = 27\n",
    "\n",
    "# Number of dimensions in vector space that we map each character to.\n",
    "embedding_dims = 3\n",
    "\n",
    "# The length of a context as a \"flattened\" array of each of its character's embeddings.\n",
    "embedded_context_dims = context_size * embedding_dims\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Matrix containing a \"lookup table\" from character indices to their embeddings in the vector space.\n",
    "C = torch.randn((vocab_size, embedding_dims), dtype=torch.float, generator=g)\n",
    "\n",
    "# Number of neurons in the hidden layer\n",
    "w1_neurons = 200\n",
    "\n",
    "# Hidden tanh layer\n",
    "W1 = torch.randn((embedded_context_dims, w1_neurons), dtype=torch.float, generator=g)\n",
    "\n",
    "b1 = torch.randn(w1_neurons, dtype=torch.float, generator=g)\n",
    "\n",
    "# Final softmax layer, scaled by 0.1 to make the initial weights be as similar to\n",
    "# each other as possible to start, thus ultimately giving each character an equal\n",
    "# probability, which results in a much better initial loss (described in beginning\n",
    "# of lecture 4).\n",
    "W2 = torch.randn((w1_neurons, vocab_size), dtype=torch.float, generator=g) * 0.1\n",
    "\n",
    "# Initialize softmax biases to 0 so every character has equal probability (see above).\n",
    "b2 = torch.randn(vocab_size, dtype=torch.float, generator=g) * 0\n",
    "\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "for param in params:\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Total parameters:\", sum([param.numel() for param in params]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15da80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    num_examples = X.shape[0]\n",
    "\n",
    "    # Each row is an example consisting of a \"flattened\" tensor of each character in the context.\n",
    "    CX = C[X].view(num_examples, embedded_context_dims)\n",
    "\n",
    "    # Make sure the very first example's first context item is the terminator character.\n",
    "    # Commenting this out b/c we want this code to be used for more than just training!\n",
    "    #terminator = C[0]\n",
    "    #assert CX[0][:embedding_dims].tolist() == terminator.tolist()\n",
    "\n",
    "    CXW1 = torch.tanh(CX @ W1 + b1)\n",
    "\n",
    "    assert list(CXW1.shape) == [num_examples, w1_neurons]\n",
    "\n",
    "    logits = CXW1 @ W2 + b2\n",
    "\n",
    "    assert list(logits.shape) == [num_examples, vocab_size]\n",
    "\n",
    "    # TODO: Use torch's softmax here to improve efficiency.\n",
    "    fake_counts = logits.exp()\n",
    "\n",
    "    probs = fake_counts / torch.sum(fake_counts, dim=1, keepdim=True)\n",
    "\n",
    "    # Ensure the probabilities of all characters in the first example sum to approximately 1.0.\n",
    "    assert probs[0].sum() - 1.0 < 0.000001\n",
    "\n",
    "    return probs\n",
    "\n",
    "def calc_loss(probs, Y):\n",
    "    num_examples = probs.shape[0]\n",
    "    assert num_examples == Y.shape[0]\n",
    "\n",
    "    # TODO: Use torch's cross-entropy loss here to improve efficiency.\n",
    "    loss = -probs[range(num_examples), Y].log().mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train(rounds=20_000, first_half_lr=0.1, second_half_lr=0.01, minibatch_size=32, X=X_train, Y=Y_train):\n",
    "    num_examples = X.shape[0]\n",
    "\n",
    "    for i in range(rounds):\n",
    "        minibatch_indexes = torch.randint(0, num_examples, (minibatch_size,))\n",
    "        minibatch = X[minibatch_indexes]\n",
    "\n",
    "        probs = forward(minibatch)\n",
    "\n",
    "        loss = calc_loss(probs, Y[minibatch_indexes])\n",
    "\n",
    "        if i % 1_000 == 0:\n",
    "            print(f\"{i:7d} / {rounds:7d} minibatch loss: {loss.item():.4f}\")\n",
    "\n",
    "        for param in params:\n",
    "            param.grad = None\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        learning_rate = first_half_lr if i < rounds / 2 else second_half_lr\n",
    "        for param in params:\n",
    "            param.data += -learning_rate * param.grad\n",
    "\n",
    "@torch.no_grad\n",
    "def calc_loss_for_dataset(X, Y):\n",
    "    return calc_loss(forward(X), Y).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506e285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0 /   20000 minibatch loss: 4.4628\n",
      "   1000 /   20000 minibatch loss: 2.5620\n",
      "   2000 /   20000 minibatch loss: 2.7277\n",
      "   3000 /   20000 minibatch loss: 2.3179\n",
      "   4000 /   20000 minibatch loss: 2.2524\n",
      "   5000 /   20000 minibatch loss: 2.3852\n",
      "   6000 /   20000 minibatch loss: 2.0266\n",
      "   7000 /   20000 minibatch loss: 2.4861\n",
      "   8000 /   20000 minibatch loss: 2.4739\n",
      "   9000 /   20000 minibatch loss: 2.3933\n",
      "  10000 /   20000 minibatch loss: 2.6492\n",
      "  11000 /   20000 minibatch loss: 2.2198\n",
      "  12000 /   20000 minibatch loss: 2.2753\n",
      "  13000 /   20000 minibatch loss: 2.1535\n",
      "  14000 /   20000 minibatch loss: 2.0532\n",
      "  15000 /   20000 minibatch loss: 2.0948\n",
      "  16000 /   20000 minibatch loss: 2.2173\n",
      "  17000 /   20000 minibatch loss: 1.9418\n",
      "  18000 /   20000 minibatch loss: 2.2555\n",
      "  19000 /   20000 minibatch loss: 2.1660\n"
     ]
    }
   ],
   "source": [
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20de5b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss over training set: 2.2239670753479004\n",
      "Final loss over dev set: 2.2493510246276855\n"
     ]
    }
   ],
   "source": [
    "print(\"Final loss over training set:\", calc_loss_for_dataset(X_train, Y_train))\n",
    "print(\"Final loss over dev set:\", calc_loss_for_dataset(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b6b414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uliona\n",
      "azra\n",
      "arrax\n",
      "jarra\n",
      "saminiteani\n",
      "piee\n",
      "parre\n",
      "kchatha\n",
      "yessy\n",
      "zaija\n"
     ]
    }
   ],
   "source": [
    "def predict(context_str='', num_chars=1000, stop_on_terminator=True, greedy=False):\n",
    "    \"\"\"\n",
    "    Given an optional starting context, predicts next character(s) in the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    while num_chars > 0:\n",
    "        context = ([0] * context_size + [stoi[ch] for ch in context_str])[-context_size:]\n",
    "        X = torch.tensor([context])\n",
    "        probs = forward(X)\n",
    "        if greedy:\n",
    "            next_idx = probs[0].argmax().item()\n",
    "        else:\n",
    "            next_idx = torch.multinomial(probs[0], 1, replacement=True).item()\n",
    "        if next_idx == 0 and stop_on_terminator:\n",
    "            break\n",
    "        context_str = context_str + itos[next_idx]\n",
    "        num_chars -= 1\n",
    "    return context_str\n",
    "\n",
    "for _ in range(10):\n",
    "    print(predict(''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1629bd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atuah\n",
      "atuella\n",
      "aturaryenson\n",
      "atulynn\n",
      "aturysyna\n",
      "atulynn\n",
      "atunan\n",
      "atulajah\n",
      "atuel\n",
      "atuk\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10): print(predict('atu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bda28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
