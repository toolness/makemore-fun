{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6b2f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "994d58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfa522e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        [ 5, 13, 13],\n",
      "        [13, 13,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0, 15],\n",
      "        [ 0, 15, 12],\n",
      "        [15, 12,  9],\n",
      "        [12,  9, 22],\n",
      "        [ 9, 22,  9],\n",
      "        [22,  9,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1, 22],\n",
      "        [ 1, 22,  1]])\n",
      "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "context_size = 3\n",
    "\n",
    "def build_xy(words):\n",
    "    \"\"\"\n",
    "    Return X, Y tuple of training data and labels given words.\n",
    "\n",
    "    X will contain one row for each example. Each example will contain `context_size`\n",
    "    elements representing character indices.\n",
    "\n",
    "    Y will contain a character index label for each example.\n",
    "    \"\"\"\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for word in words:\n",
    "        context = [0] * context_size\n",
    "        for ch in word:\n",
    "            ich = stoi[ch]\n",
    "            xs.append(context)\n",
    "            ys.append(ich)\n",
    "            context = context[1:] + [ich]\n",
    "        xs.append(context)\n",
    "        ys.append(0)\n",
    "    assert len(xs) == len(ys)\n",
    "    X = torch.tensor(xs)\n",
    "    Y = torch.tensor(ys)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = build_xy(words[:3])\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a686617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_examples = Y.shape[0]\n",
    "\n",
    "# Number of characters in our alphabet (the very first one is the terminator character).\n",
    "vocab_size = 27\n",
    "\n",
    "# Number of dimensions in vector space that we map each character to.\n",
    "embedding_dims = 2\n",
    "\n",
    "# The length of a context as a \"flattened\" array of each of its character's embeddings.\n",
    "embedded_context_dims = context_size * embedding_dims\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Matrix containing a \"lookup table\" from character indices to their embeddings in the vector space.\n",
    "C = torch.randn((vocab_size, embedding_dims), dtype=torch.float, generator=g)\n",
    "\n",
    "# Number of neurons in the hidden layer\n",
    "w1_neurons = 10\n",
    "\n",
    "# Hidden tanh layer\n",
    "W1 = torch.randn((embedded_context_dims, w1_neurons), dtype=torch.float, generator=g)\n",
    "\n",
    "# Final softmax layer\n",
    "W2 = torch.randn((w1_neurons, vocab_size), dtype=torch.float, generator=g)\n",
    "\n",
    "params = [C, W1, W2]\n",
    "\n",
    "for param in params:\n",
    "    param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15da80ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:  0.46320825815200806\n",
      "LOSS:  0.45963940024375916\n",
      "LOSS:  0.45616501569747925\n",
      "LOSS:  0.4527813196182251\n",
      "LOSS:  0.44948506355285645\n",
      "LOSS:  0.4462727904319763\n",
      "LOSS:  0.44314125180244446\n",
      "LOSS:  0.4400876462459564\n",
      "LOSS:  0.4371088743209839\n",
      "LOSS:  0.43420249223709106\n",
      "LOSS:  0.4313656687736511\n",
      "LOSS:  0.4285960793495178\n",
      "LOSS:  0.425891250371933\n",
      "LOSS:  0.4232490062713623\n",
      "LOSS:  0.4206671416759491\n",
      "LOSS:  0.41814374923706055\n",
      "LOSS:  0.4156767427921295\n",
      "LOSS:  0.4132642149925232\n",
      "LOSS:  0.4109044671058655\n",
      "LOSS:  0.4085957705974579\n",
      "LOSS:  0.4063364863395691\n",
      "LOSS:  0.4041249752044678\n",
      "LOSS:  0.40195977687835693\n",
      "LOSS:  0.39983952045440674\n",
      "LOSS:  0.3977626860141754\n",
      "LOSS:  0.3957280218601227\n",
      "LOSS:  0.393734335899353\n",
      "LOSS:  0.39178016781806946\n",
      "LOSS:  0.38986462354660034\n",
      "LOSS:  0.38798633217811584\n",
      "LOSS:  0.38614434003829956\n",
      "LOSS:  0.3843376338481903\n",
      "LOSS:  0.38256514072418213\n",
      "LOSS:  0.38082587718963623\n",
      "LOSS:  0.379118949174881\n",
      "LOSS:  0.37744346261024475\n",
      "LOSS:  0.3757985532283783\n",
      "LOSS:  0.3741833567619324\n",
      "LOSS:  0.37259721755981445\n",
      "LOSS:  0.37103918194770813\n",
      "LOSS:  0.3695084750652313\n",
      "LOSS:  0.36800456047058105\n",
      "LOSS:  0.3665265440940857\n",
      "LOSS:  0.3650740385055542\n",
      "LOSS:  0.36364611983299255\n",
      "LOSS:  0.36224234104156494\n",
      "LOSS:  0.3608619272708893\n",
      "LOSS:  0.3595044016838074\n",
      "LOSS:  0.35816922783851624\n",
      "LOSS:  0.3568558692932129\n",
      "LOSS:  0.3555636703968048\n",
      "LOSS:  0.3542921841144562\n",
      "LOSS:  0.35304099321365356\n",
      "LOSS:  0.3518095314502716\n",
      "LOSS:  0.3505973517894745\n",
      "LOSS:  0.3494040071964264\n",
      "LOSS:  0.3482291102409363\n",
      "LOSS:  0.34707218408584595\n",
      "LOSS:  0.34593284130096436\n",
      "LOSS:  0.3448106646537781\n",
      "LOSS:  0.34370532631874084\n",
      "LOSS:  0.34261637926101685\n",
      "LOSS:  0.3415435254573822\n",
      "LOSS:  0.3404863476753235\n",
      "LOSS:  0.3394445776939392\n",
      "LOSS:  0.33841782808303833\n",
      "LOSS:  0.3374057710170746\n",
      "LOSS:  0.33640816807746887\n",
      "LOSS:  0.3354246914386749\n",
      "LOSS:  0.3344550132751465\n",
      "LOSS:  0.33349886536598206\n",
      "LOSS:  0.3325559198856354\n",
      "LOSS:  0.3316260278224945\n",
      "LOSS:  0.3307088315486908\n",
      "LOSS:  0.3298041820526123\n",
      "LOSS:  0.32891154289245605\n",
      "LOSS:  0.328031063079834\n",
      "LOSS:  0.3271622359752655\n",
      "LOSS:  0.32630497217178345\n",
      "LOSS:  0.3254588842391968\n",
      "LOSS:  0.3246239721775055\n",
      "LOSS:  0.32379984855651855\n",
      "LOSS:  0.32298651337623596\n",
      "LOSS:  0.3221834897994995\n",
      "LOSS:  0.3213907778263092\n",
      "LOSS:  0.32060810923576355\n",
      "LOSS:  0.3198353350162506\n",
      "LOSS:  0.31907224655151367\n",
      "LOSS:  0.3183186650276184\n",
      "LOSS:  0.31757447123527527\n",
      "LOSS:  0.31683945655822754\n",
      "LOSS:  0.3161134719848633\n",
      "LOSS:  0.3153962790966034\n",
      "LOSS:  0.3146878480911255\n",
      "LOSS:  0.3139879107475281\n",
      "LOSS:  0.31329643726348877\n",
      "LOSS:  0.31261321902275085\n",
      "LOSS:  0.3119381070137024\n",
      "LOSS:  0.31127089262008667\n",
      "LOSS:  0.3106116056442261\n"
     ]
    }
   ],
   "source": [
    "num_training_rounds = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "def forward(X):\n",
    "    num_examples = X.shape[0]\n",
    "\n",
    "    # Each row is an example consisting of a \"flattened\" tensor of each character in the context.\n",
    "    CX = C[X].view(num_examples, embedded_context_dims)\n",
    "\n",
    "    # Make sure the very first example's first context item is the terminator character.\n",
    "    terminator = C[0]\n",
    "    assert CX[0][:embedding_dims].tolist() == terminator.tolist()\n",
    "\n",
    "    CXW1 = torch.tanh(CX @ W1)\n",
    "\n",
    "    assert list(CXW1.shape) == [num_examples, w1_neurons]\n",
    "\n",
    "    logits = CXW1 @ W2\n",
    "\n",
    "    assert list(logits.shape) == [num_examples, vocab_size]\n",
    "\n",
    "    # TODO: Use torch's softmax here to improve efficiency.\n",
    "    fake_counts = logits.exp()\n",
    "\n",
    "    probs = fake_counts / torch.sum(fake_counts, dim=1, keepdim=True)\n",
    "\n",
    "    # Ensure the probabilities of all characters in the first example sum to approximately 1.0.\n",
    "    assert probs[0].sum() - 1.0 < 0.000001\n",
    "\n",
    "    return probs\n",
    "\n",
    "for i in range(num_training_rounds):\n",
    "    probs = forward(X)\n",
    "\n",
    "    # TODO: Use torch's cross-entropy loss here to improve efficiency.\n",
    "    loss = -probs[range(num_examples), Y].log().mean()\n",
    "\n",
    "    print(\"LOSS: \", loss.item())\n",
    "\n",
    "    for param in params:\n",
    "        param.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    for param in params:\n",
    "        param.data += -learning_rate * param.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84b6b414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2842, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should be relatively high, because 'emma' is in the training set.\n",
    "forward(torch.tensor([[0, 0, 0]]))[0][stoi['e']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0d00f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
