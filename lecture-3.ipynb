{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b2f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994d58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa522e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        [ 5, 13, 13],\n",
      "        [13, 13,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0, 15],\n",
      "        [ 0, 15, 12],\n",
      "        [15, 12,  9],\n",
      "        [12,  9, 22],\n",
      "        [ 9, 22,  9],\n",
      "        [22,  9,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1, 22],\n",
      "        [ 1, 22,  1]])\n",
      "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "context_size = 3\n",
    "\n",
    "def build_xy(words):\n",
    "    \"\"\"\n",
    "    Return X, Y tuple of training data and labels given words.\n",
    "\n",
    "    X will contain one row for each example. Each example will contain `context_size`\n",
    "    elements representing character indices.\n",
    "\n",
    "    Y will contain a character index label for each example.\n",
    "    \"\"\"\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for word in words:\n",
    "        context = [0] * context_size\n",
    "        for ch in word:\n",
    "            ich = stoi[ch]\n",
    "            xs.append(context)\n",
    "            ys.append(ich)\n",
    "            context = context[1:] + [ich]\n",
    "        xs.append(context)\n",
    "        ys.append(0)\n",
    "    assert len(xs) == len(ys)\n",
    "    X = torch.tensor(xs)\n",
    "    Y = torch.tensor(ys)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = build_xy(words[:3])\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a686617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_examples = Y.shape[0]\n",
    "\n",
    "# Number of characters in our alphabet (the very first one is the terminator character).\n",
    "vocab_size = 27\n",
    "\n",
    "# Number of dimensions in vector space that we map each character to.\n",
    "embedding_dims = 2\n",
    "\n",
    "# The length of a context as a \"flattened\" array of each of its character's embeddings.\n",
    "embedded_context_dims = context_size * embedding_dims\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Matrix containing a \"lookup table\" from character indices to their embeddings in the vector space.\n",
    "C = torch.randn((vocab_size, embedding_dims), dtype=torch.float, generator=g)\n",
    "\n",
    "# Number of neurons in the hidden layer\n",
    "w1_neurons = 10\n",
    "\n",
    "# Hidden tanh layer\n",
    "W1 = torch.randn((embedded_context_dims, w1_neurons), dtype=torch.float, generator=g)\n",
    "\n",
    "# Final softmax layer\n",
    "W2 = torch.randn((w1_neurons, vocab_size), dtype=torch.float, generator=g)\n",
    "\n",
    "params = [C, W1, W2]\n",
    "\n",
    "for param in params:\n",
    "    param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15da80ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:  6.117327690124512\n",
      "LOSS:  5.758670330047607\n",
      "LOSS:  5.432277202606201\n",
      "LOSS:  5.146899700164795\n",
      "LOSS:  4.900434970855713\n",
      "LOSS:  4.682767391204834\n",
      "LOSS:  4.48641300201416\n",
      "LOSS:  4.306982517242432\n",
      "LOSS:  4.140802383422852\n",
      "LOSS:  3.9846444129943848\n",
      "LOSS:  3.8361637592315674\n",
      "LOSS:  3.6939735412597656\n",
      "LOSS:  3.557457447052002\n",
      "LOSS:  3.4264895915985107\n",
      "LOSS:  3.301115036010742\n",
      "LOSS:  3.1812713146209717\n",
      "LOSS:  3.066671371459961\n",
      "LOSS:  2.9568614959716797\n",
      "LOSS:  2.8513476848602295\n",
      "LOSS:  2.7496840953826904\n",
      "LOSS:  2.65151309967041\n",
      "LOSS:  2.5565662384033203\n",
      "LOSS:  2.4646553993225098\n",
      "LOSS:  2.3756566047668457\n",
      "LOSS:  2.289499044418335\n",
      "LOSS:  2.206149101257324\n",
      "LOSS:  2.125601291656494\n",
      "LOSS:  2.0478672981262207\n",
      "LOSS:  1.972965955734253\n",
      "LOSS:  1.9009182453155518\n",
      "LOSS:  1.831740140914917\n",
      "LOSS:  1.7654387950897217\n",
      "LOSS:  1.7020087242126465\n",
      "LOSS:  1.641431212425232\n",
      "LOSS:  1.5836721658706665\n",
      "LOSS:  1.5286822319030762\n",
      "LOSS:  1.4763970375061035\n",
      "LOSS:  1.4267381429672241\n",
      "LOSS:  1.3796145915985107\n",
      "LOSS:  1.3349249362945557\n",
      "LOSS:  1.2925596237182617\n",
      "LOSS:  1.2524034976959229\n",
      "LOSS:  1.2143374681472778\n",
      "LOSS:  1.1782419681549072\n",
      "LOSS:  1.1439987421035767\n",
      "LOSS:  1.1114915609359741\n",
      "LOSS:  1.0806092023849487\n",
      "LOSS:  1.0512454509735107\n",
      "LOSS:  1.0233006477355957\n",
      "LOSS:  0.9966817498207092\n",
      "LOSS:  0.9713033437728882\n",
      "LOSS:  0.9470876455307007\n",
      "LOSS:  0.9239646196365356\n",
      "LOSS:  0.9018716812133789\n",
      "LOSS:  0.8807530403137207\n",
      "LOSS:  0.8605600595474243\n",
      "LOSS:  0.8412491679191589\n",
      "LOSS:  0.8227822184562683\n",
      "LOSS:  0.8051241040229797\n",
      "LOSS:  0.7882423996925354\n",
      "LOSS:  0.7721065878868103\n",
      "LOSS:  0.7566866278648376\n",
      "LOSS:  0.7419530749320984\n",
      "LOSS:  0.7278760075569153\n",
      "LOSS:  0.7144257426261902\n",
      "LOSS:  0.7015724778175354\n",
      "LOSS:  0.6892864108085632\n",
      "LOSS:  0.6775386333465576\n",
      "LOSS:  0.6663004755973816\n",
      "LOSS:  0.6555445194244385\n",
      "LOSS:  0.6452443599700928\n",
      "LOSS:  0.6353747248649597\n",
      "LOSS:  0.6259118318557739\n",
      "LOSS:  0.6168332099914551\n",
      "LOSS:  0.6081175804138184\n",
      "LOSS:  0.5997451543807983\n",
      "LOSS:  0.5916973352432251\n",
      "LOSS:  0.5839564800262451\n",
      "LOSS:  0.5765063166618347\n",
      "LOSS:  0.5693315267562866\n",
      "LOSS:  0.5624179244041443\n",
      "LOSS:  0.5557519197463989\n",
      "LOSS:  0.5493209958076477\n",
      "LOSS:  0.5431134700775146\n",
      "LOSS:  0.537118136882782\n",
      "LOSS:  0.5313247442245483\n",
      "LOSS:  0.5257235169410706\n",
      "LOSS:  0.5203052759170532\n",
      "LOSS:  0.5150614380836487\n",
      "LOSS:  0.5099838972091675\n",
      "LOSS:  0.5050650835037231\n",
      "LOSS:  0.5002977848052979\n",
      "LOSS:  0.4956752061843872\n",
      "LOSS:  0.49119091033935547\n",
      "LOSS:  0.4868389964103699\n",
      "LOSS:  0.48261380195617676\n",
      "LOSS:  0.47850972414016724\n",
      "LOSS:  0.4745219051837921\n",
      "LOSS:  0.47064530849456787\n",
      "LOSS:  0.46687552332878113\n"
     ]
    }
   ],
   "source": [
    "num_training_rounds = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "def forward(X):\n",
    "    num_examples = X.shape[0]\n",
    "\n",
    "    # Each row is an example consisting of a \"flattened\" tensor of each character in the context.\n",
    "    CX = C[X].view(num_examples, embedded_context_dims)\n",
    "\n",
    "    # Make sure the very first example's first context item is the terminator character.\n",
    "    # Commenting this out b/c we want this code to be used for more than just training!\n",
    "    #terminator = C[0]\n",
    "    #assert CX[0][:embedding_dims].tolist() == terminator.tolist()\n",
    "\n",
    "    CXW1 = torch.tanh(CX @ W1)\n",
    "\n",
    "    assert list(CXW1.shape) == [num_examples, w1_neurons]\n",
    "\n",
    "    logits = CXW1 @ W2\n",
    "\n",
    "    assert list(logits.shape) == [num_examples, vocab_size]\n",
    "\n",
    "    # TODO: Use torch's softmax here to improve efficiency.\n",
    "    fake_counts = logits.exp()\n",
    "\n",
    "    probs = fake_counts / torch.sum(fake_counts, dim=1, keepdim=True)\n",
    "\n",
    "    # Ensure the probabilities of all characters in the first example sum to approximately 1.0.\n",
    "    assert probs[0].sum() - 1.0 < 0.000001\n",
    "\n",
    "    return probs\n",
    "\n",
    "for i in range(num_training_rounds):\n",
    "    probs = forward(X)\n",
    "\n",
    "    # TODO: Use torch's cross-entropy loss here to improve efficiency.\n",
    "    loss = -probs[range(num_examples), Y].log().mean()\n",
    "\n",
    "    print(\"LOSS: \", loss.item())\n",
    "\n",
    "    for param in params:\n",
    "        param.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    for param in params:\n",
    "        param.data += -learning_rate * param.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b6b414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emma...ava...ava...ava...ava...ava...ava...ava...ava...ava...ava...ava...ava...ava...ava...ava...ava.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_greedy(context_str='', num_chars=1):\n",
    "    \"\"\"\n",
    "    Given a starting context, returns the context with the most likely next characters.\n",
    "    \"\"\"\n",
    "\n",
    "    context = ([0] * context_size + [stoi[ch] for ch in context_str])[-context_size:]\n",
    "    X = torch.tensor([context])\n",
    "    probs = forward(X)\n",
    "    new_context_str = context_str + itos[probs[0].argmax().item()]\n",
    "    num_chars -= 1\n",
    "    if num_chars == 0:\n",
    "        return new_context_str\n",
    "    return predict_greedy(new_context_str, num_chars)\n",
    "\n",
    "# This is funny\n",
    "predict_greedy('e', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0d00f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
